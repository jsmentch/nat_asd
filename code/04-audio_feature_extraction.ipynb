{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forbidden-indie",
   "metadata": {},
   "source": [
    "# extract auditory features from audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use conda env cochdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pleasant-termination",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_918085/2211653933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrobustness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_audio_wav_resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#from analysis_scripts.default_paths import fMRI_DATA_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# make sure we are using the correct plotting display. \n",
    "import matplotlib \n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "if sys.version_info < (3,):\n",
    "    from StringIO import StringIO as BytesIO\n",
    "else:\n",
    "    from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import scipy\n",
    "import pickle\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from robustness.tools.audio_helpers import load_audio_wav_resample\n",
    "#from analysis_scripts.default_paths import fMRI_DATA_PATH\n",
    "\n",
    "import itertools\n",
    "\n",
    "#import build_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_sound_np(sound):\n",
    "    sound = sound - np.mean(sound)\n",
    "    sound = sound/np.sqrt(np.mean(sound**2))*0.1\n",
    "    sound = np.expand_dims(sound, 0)\n",
    "    sound = torch.from_numpy(sound).float().cuda()\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corporate-cheese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/net/vast-storage.ib.cluster/scratch/vast/gablab/jsmentch/cochdnn/model_checkpoints/audio_rep_training_cochleagram_1/standard_training_word_and_audioset_and_speaker_decay_lr/542752d7-9849-49ff-b84a-6758a81585b4/5_checkpoint.pt'\n",
      "=> loaded checkpoint '/net/vast-storage.ib.cluster/scratch/vast/gablab/jsmentch/cochdnn/model_checkpoints/audio_rep_training_cochleagram_1/standard_training_word_and_audioset_and_speaker_decay_lr/542752d7-9849-49ff-b84a-6758a81585b4/5_checkpoint.pt' (epoch 6)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Choose the model that will be loaded\n",
    "model_dir = '/om2/user/jsmentch/cochdnn/model_directories/resnet50_word_speaker_audioset'\n",
    "\n",
    "build_network_spec = importlib.util.spec_from_file_location(\"build_network\",\n",
    "                        os.path.join(model_dir, 'build_network.py'))\n",
    "build_network = importlib.util.module_from_spec(build_network_spec)\n",
    "build_network_spec.loader.exec_module(build_network)\n",
    "\n",
    "model, ds, all_layers = build_network.main(return_metamer_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interracial-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40925/6305016.py:26: RuntimeWarning: invalid value encountered in divide\n",
      "  wav_array[wav_idx,:] = test_audio/np.sqrt(np.mean(test_audio**2))\n",
      "/tmp/ipykernel_40925/6305016.py:26: RuntimeWarning: invalid value encountered in sqrt\n",
      "  wav_array[wav_idx,:] = test_audio/np.sqrt(np.mean(test_audio**2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_after_preproc\n",
      "conv1\n",
      "bn1\n",
      "conv1_relu1\n",
      "maxpool1\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "final/signal/word_int\n",
      "final/signal/speaker_int\n",
      "final/noise/labels_binary_via_int\n",
      "input_after_preproc\n",
      "conv1\n",
      "bn1\n",
      "conv1_relu1\n",
      "maxpool1\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "final/signal/word_int\n",
      "final/signal/speaker_int\n",
      "final/noise/labels_binary_via_int\n",
      "input_after_preproc\n",
      "conv1\n",
      "bn1\n",
      "conv1_relu1\n",
      "maxpool1\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "final/signal/word_int\n",
      "final/signal/speaker_int\n",
      "final/noise/labels_binary_via_int\n",
      "input_after_preproc\n",
      "conv1\n",
      "bn1\n",
      "conv1_relu1\n",
      "maxpool1\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "final/signal/word_int\n",
      "final/signal/speaker_int\n",
      "final/noise/labels_binary_via_int\n"
     ]
    }
   ],
   "source": [
    "##############Begin Define Parameters#################\n",
    "stim='TP'\n",
    "for stim in ['friends_s01e01a','friends_s01e01b','friends_s01e02a','friends_s01e02b']:\n",
    "\n",
    "    save_features_dir = f'../data/{stim}_clips_cochresnet50/'\n",
    "    input_dir = f'../data/{stim}_clips/'\n",
    "    \n",
    "    \n",
    "    if not os.path.isdir(save_features_dir):\n",
    "        os.mkdir(save_features_dir) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############LOAD_AUDIO################\n",
    "    # contains the metatdata for the list of presented sounds (should be in the correct order)\n",
    "    #sound_list = np.load(os.path.join(fMRI_DATA_PATH, 'neural_stim_meta.npy'))\n",
    "    sound_list = glob.glob(f'{input_dir}*.wav')\n",
    "    sound_list_n=len(sound_list)\n",
    "    #wavs_location = os.path.join(fMRI_DATA_PATH, '165_natural_sounds')\n",
    "    \n",
    "    SR=20000 # Match with the networks we are building/training\n",
    "    MEASURE_DUR=2\n",
    "    wav_array = np.empty([sound_list_n, SR*MEASURE_DUR])\n",
    "    for wav_idx, wav_data in enumerate(sound_list):\n",
    "        test_audio, SR = load_audio_wav_resample(wav_data, DUR_SECS=MEASURE_DUR, resample_SR=SR)\n",
    "        wav_array[wav_idx,:] = test_audio/np.sqrt(np.mean(test_audio**2))\n",
    "    \n",
    "    # Measure the activations for each sound for each layer, and put the input in the dictionary array. \n",
    "\n",
    "    filename = 'cochresnet50_activations'\n",
    "    # only use the non-fake layers\n",
    "    all_layers = [e.split('_fake')[0] for e in all_layers] # Don't duplicate these since we aren't synthesizing\n",
    "    new_all_layers = []\n",
    "    for l_unique in all_layers:\n",
    "        if l_unique not in new_all_layers:\n",
    "            new_all_layers.append(l_unique)\n",
    "    all_layers = new_all_layers\n",
    "    net_layer_dict = {}\n",
    "    net_layer_dict_full = {}\n",
    "    net_h5py_file = h5py.File(os.path.join(save_features_dir, filename + '.h5'), \"w\")\n",
    "    net_h5py_file_full = h5py.File(os.path.join(save_features_dir, filename + '_full.h5'), \"w\")\n",
    "    \n",
    "    # Save the list of layers to the hdf5\n",
    "    net_h5py_file['layer_list'] = np.array([layer.encode(\"utf-8\") for layer in all_layers])\n",
    "    net_h5py_file_full['layer_list'] = np.array([layer.encode(\"utf-8\") for layer in all_layers])\n",
    "    \n",
    "    for sound_idx, sound_info in enumerate(sound_list):\n",
    "        sound = preproc_sound_np(wav_array[sound_idx,:])\n",
    "        with torch.no_grad():\n",
    "            (predictions, rep, layer_returns), orig_image = model(sound, with_latent=True) # Corresponding representation\n",
    "    \n",
    "        # Make the array have the correct size\n",
    "        if sound_idx == 0:\n",
    "            for layer in all_layers:\n",
    "                print(layer)\n",
    "                layer_shape_165 = layer_returns[layer].shape\n",
    "                layer_shape_full = np.prod(np.array(layer_shape_165))\n",
    "                if len(layer_shape_165)==4:\n",
    "                    layer_shape_unraveled = layer_shape_165[1]*layer_shape_165[2]# don't take the time dimension into account\n",
    "                else:\n",
    "                    layer_shape_unraveled = layer_shape_165[1]\n",
    "                net_layer_dict_full[layer] = net_h5py_file_full.create_dataset(layer, (sound_list_n, layer_shape_full), dtype='float32')\n",
    "                net_layer_dict[layer] = net_h5py_file.create_dataset(layer, (sound_list_n, layer_shape_unraveled), dtype='float32')\n",
    "    \n",
    "        for layer_idx, layer in enumerate(all_layers):\n",
    "            # time averaged features, so that they can be related to the fMRI activations\n",
    "            if layer_returns[layer].ndim==4: # NCHW (W is time)\n",
    "                net_layer_dict[layer][sound_idx,:] = np.mean(layer_returns[layer].cpu().detach().numpy(),3).ravel()\n",
    "            else: # fully connected layers do not have a temporal component.  \n",
    "                net_layer_dict[layer][sound_idx,:] = layer_returns[layer].cpu().detach().numpy().ravel()\n",
    "            net_layer_dict_full[layer][sound_idx,:] = layer_returns[layer].cpu().detach().numpy().ravel()\n",
    "    net_h5py_file.close()\n",
    "    net_h5py_file_full.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abroad-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_after_preproc',\n",
       " 'conv1',\n",
       " 'bn1',\n",
       " 'conv1_relu1',\n",
       " 'maxpool1',\n",
       " 'layer1',\n",
       " 'layer2',\n",
       " 'layer3',\n",
       " 'layer4',\n",
       " 'avgpool',\n",
       " 'final/signal/word_int',\n",
       " 'final/signal/speaker_int',\n",
       " 'final/noise/labels_binary_via_int']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "realistic-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_after_preproc\n",
      "conv1\n",
      "bn1\n",
      "conv1_relu1\n",
      "maxpool1\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "final/signal/word_int\n",
      "final/signal/speaker_int\n",
      "final/noise/labels_binary_via_int\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ian: Another slight detail. You’ll notice there are two h5 files being made to save the layer embeddings:\n",
    "net_h5py_file_full and net_h5py_file. For different reasons, Jenelle would often work with time-averaged \n",
    "representations but saved the time-averaged and full ones separately just in case (to avoid having to\n",
    "average every time). Full is the full embedding and the other is the time averaged one.  If you’re ever\n",
    "confused in this repo, representations, activations, and embeddings all mean the same thing here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-department",
   "metadata": {},
   "source": [
    "## look at the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers=['input_after_preproc',\n",
    " 'conv1',\n",
    " 'bn1',\n",
    " 'conv1_relu1',\n",
    " 'maxpool1',\n",
    " 'layer1',\n",
    " 'layer2',\n",
    " 'layer3',\n",
    " 'layer4',\n",
    " 'avgpool',\n",
    " 'final/signal/word_int',\n",
    " 'final/signal/speaker_int',\n",
    " 'final/noise/labels_binary_via_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CochResNet50 time-averaged\n",
      "(749, 211) input_after_preproc\n",
      "(749, 6784) conv1\n",
      "(749, 6784) bn1\n",
      "(749, 6784) conv1_relu1\n",
      "(749, 3392) maxpool1\n",
      "(749, 13568) layer1\n",
      "(749, 13824) layer2\n",
      "(749, 14336) layer3\n",
      "(749, 14336) layer4\n",
      "(749, 2048) avgpool\n",
      "(749, 794) final/signal/word_int\n",
      "(749, 433) final/signal/speaker_int\n",
      "(749, 517) final/noise/labels_binary_via_int\n",
      "CochResNet50 full\n",
      "(749, 82290) input_after_preproc\n",
      "(749, 1322880) conv1\n",
      "(749, 1322880) bn1\n",
      "(749, 1322880) conv1_relu1\n",
      "(749, 332416) maxpool1\n",
      "(749, 1329664) layer1\n",
      "(749, 677376) layer2\n",
      "(749, 358400) layer3\n",
      "(749, 186368) layer4\n",
      "(749, 2048) avgpool\n",
      "(749, 794) final/signal/word_int\n",
      "(749, 433) final/signal/speaker_int\n",
      "(749, 517) final/noise/labels_binary_via_int\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "stim='DM'\n",
    "save_features_dir = f'../data/{stim}_clips_cochresnet50/'\n",
    "\n",
    "print('CochResNet50 time-averaged')\n",
    "# Open the file 'myfile.h5' in read-only mode\n",
    "file = h5py.File(f'{save_features_dir}cochresnet50_activations.h5', 'r')\n",
    "for layer in all_layers:\n",
    "# # Now you can access datasets within the file\n",
    "    data = file[layer]\n",
    "    print(data.shape, layer)\n",
    "# # Don't forget to close the file when you're done\n",
    "file.close()\n",
    "\n",
    "print('CochResNet50 full')\n",
    "# Open the file 'myfile.h5' in read-only mode\n",
    "file = h5py.File(f'{save_features_dir}cochresnet50_activations_full.h5', 'r')\n",
    "for layer in all_layers:\n",
    "# # Now you can access datasets within the file\n",
    "    data = file[layer]\n",
    "    print(data.shape, layer)\n",
    "# # Don't forget to close the file when you're done\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preceding-tolerance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82290,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "CochResNet50 time-averaged\n",
    "(749, 211) input_after_preproc (cochleagram)\n",
    "(749, 6784) conv1\n",
    "(749, 6784) bn1\n",
    "(749, 6784) conv1_relu1\n",
    "(749, 3392) maxpool1\n",
    "(749, 13568) layer1\n",
    "(749, 13824) layer2\n",
    "(749, 14336) layer3\n",
    "(749, 14336) layer4\n",
    "(749, 2048) avgpool\n",
    "(749, 794) final/signal/word_int\n",
    "(749, 433) final/signal/speaker_int\n",
    "(749, 517) final/noise/labels_binary_via_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-canberra",
   "metadata": {},
   "source": [
    "## pyloudnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electrical-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pyloudnorm as pyln\n",
    "import glob\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorrect-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim='DM'\n",
    "stim='TP'\n",
    "\n",
    "# Load and normalize the audio data\n",
    "data, rate = librosa.load(f'../data/{stim}.wav', sr=20000)\n",
    "data = data / np.max(np.abs(data))  # Normalizing audio\n",
    "\n",
    "# Define the window size (800 ms) in terms of samples\n",
    "window_size = int(0.800 * rate)\n",
    "\n",
    "# Create a loudness meter\n",
    "meter = pyln.Meter(rate)\n",
    "\n",
    "# Initialize lists to store loudness and RMS values\n",
    "loudness_values = []\n",
    "rms_values = []\n",
    "\n",
    "# Loop through the data in 800 ms chunks\n",
    "for i in range(0, len(data), window_size):\n",
    "    chunk = data[i:i + window_size]\n",
    "    \n",
    "    if len(chunk) < window_size:\n",
    "        break\n",
    "    \n",
    "    # Calculate RMS\n",
    "    rms = np.sqrt(np.mean(chunk**2))\n",
    "    \n",
    "    # Store the RMS value\n",
    "    rms_values.append(rms)\n",
    "    \n",
    "    # Only calculate loudness if RMS exceeds the threshold\n",
    "    loudness = meter.integrated_loudness(chunk)\n",
    "    \n",
    "    \n",
    "    # Store the loudness value\n",
    "    loudness_values.append(loudness)\n",
    "\n",
    "# Convert lists to numpy arrays for further processing\n",
    "loudness_array = np.array(loudness_values)\n",
    "rms_array = np.array(rms_values)\n",
    "\n",
    "# Print or compare the loudness and RMS arrays\n",
    "# print(\"Loudness (LUFS):\", loudness_array)\n",
    "# print(\"RMS:\", rms_array)\n",
    "np.save(f'../data/features/{stim}_lla_rms', rms_array)\n",
    "np.save(f'../data/features/{stim}_lla_lufs', loudness_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-residence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
